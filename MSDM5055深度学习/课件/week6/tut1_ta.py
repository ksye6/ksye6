# -*- coding: utf-8 -*-
"""tut1_TA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xMsbeVn-Ahwc7QxFW7iJvYeJKiihEMRZ

# Tutorial 1 of DDM5005
Yaowen ZHANG(yaowen.zhang@connect.ust.hk)

This tutorial is about basic applications of pytorch. [Pytorch](https://pytorch.org/tutorials/) is a deep learning library developed by Meta AI. It utilizes computational graph and autodiff to build and train the neural network.

'device' is where to train the model and load the data. The pytorch has three available device options.

GPU and CPU are two most common devices. MPS are designed for mac os. If you have multiple GPUs, you can use the following way to distinguish them, like

'cuda:0', 'cuda:1', 'cuda:2'......
"""

import torch
print('torch version:',torch.__version__)
# Get cpu, gpu or mps device for training.
device = (
    "cuda"
    if torch.cuda.is_available()
    else "cpu"
)
print('Using device:', device)

"""### concept ###
tensor can be seen as a multidimensional array. For instance, vector is a tenor with dimensional 1. Matrix is a tensor with dimensian 2.
"""

a = torch.randn(2, 3)
# each entry is an normal distribution
print('tensor a:', a)
print('a.shape:', a.shape)
print('a.dtype:', a.dtype)
print('a.device', a.device)
a = a.to(torch.float64)
print('tensor a:', a)
print('a.dtype:', a.dtype)

a = torch.randn(2,3, dtype=torch.float64)
print('a.mean:', torch.mean(a), a.mean())
print('a.var:', torch.var(a), a.var())

"""### Initialize a tensor ###

"""

# create a tensor with all zero entry
a = torch.zeros((2,3))
print('a:', a)
print('a.dtype', a.dtype)

# create a tensor with all one entry
a = torch.ones((2, 2), dtype=torch.float16)
print('a:', a)

b = torch.zeros_like(a)
print('b:', b)

b = torch.Tensor.new(a)
print('b.shape', b.shape)
print('b:', b)      # b is empty, but has the same properity as a

import numpy as np
a = np.array([[1, 2], [3, 4]])
print(a)
b = torch.from_numpy(a)
print(b)
print(b.numpy())

a = np.linspace(start=0, stop=1, num=5)
print(a)
b = torch.linspace(start=0, end=1, steps=5)
print(b)
a = np.arange(start=0, stop=5)
print(a)
b = torch.arange(start=0, end=5)
print(b)

a = torch.tensor([[1, 2, 3], [4, 5, 6]])
print('a.shape', a.shape,'\n a:', a)
b = a.view(3, 2)
print('a:', a)
print('b:', b)
b[0, 0] = 0
print('b:', b)
print('a:', a)

b= (a.t().contiguous()).view(1, 6)
print(b)
b = (a.t()).view(1, 6)

"""'view'ï¼› the two variable shares the same memory. It has the contiguity constraints

"""

a = torch.tensor([[1, 2, 3], [4, 5, 6]])
print('a.shape', a.shape,'\n a:', a)
b = a.reshape(3, 2)
print('a:', a)
print('b:', b)
b[0, 0] = 0
print('b:', b)
print('a:', a)

b = (a.t()).reshape(1, 6)
print('b:', b)
print('a:', a)

b[0,0] = 1
print('b:', b)
print('a:', a)

"""## operation ##
element-wise product and tensor product
"""

a = torch.tensor([[1, 2], [3, 4]])
b = torch.tensor([[1, 1], [2, 2]])
print('a:', a)
print('b:', b)
print('element-wise product:\n', a*b)
print('matrix product:\n', torch.matmul(a, b))

"""broadcast mechanism"""

a = torch.tensor([[1, 2], [3, 4]])
b = torch.tensor([1, 2])
print(a.shape,b.shape)
print(a, b)
print(a*b)

"""min, max, sum"""

a = torch.randn(2, 3)
print(a)
print(torch.max(a), a.max())
print(a.sum(dim=0), a.sum(dim=-1))

a = torch.randn(2, 3)
print(a)
print(a[0, 0])
print(a[1,:])
print(a.reshape(-1))              # flatten the tensor
print(a.reshape(1, 2, 3))         # add dimension

b = torch.randn(1, 3)
print(torch.cat((a, b), dim=0))
print(torch.cat((a, b, b), dim=0))

"""## copy problem ##
The key is to see whether a new memory is created and assigned


"""

a = torch.tensor([[1, 2, 3], [4, 5, 6]])
print(a.shape, a)
b = a[0,:]
print(b.shape, b)
b[0] = 0
print(b)
print(a)
# shallow copy, a and b share the same memory

a = torch.tensor([[1, 2, 3], [4, 5, 6]])
print(a.shape, a)
b = a[0,:].clone()
print(b.shape, b)
b[0] = 0
print(b)
print(a)

"""In-place operation is an operation that changes directly the content of a given linear algebra, vector, matrices(Tensor) without making a copy.

avoid inplace operation in pytorch since it will break the computation graph,

"""

from torch import autograd
a = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32,  requires_grad=True)
y = torch.sum(a)

grads = autograd.grad(outputs=y, inputs=a)[0]
print(grads)

"""inplace operation to replace one entery of a"""

a = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32,  requires_grad=True)
a[0, 0] = 0
y = torch.sum(a)

grads = autograd.grad(outputs=y, inputs=a)[0]
print(grads)

a = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32,  requires_grad=True)
mask = torch.ones_like(a)
mask[0,0] = 0
y = torch.sum(a*mask)

grads = autograd.grad(outputs=y, inputs=a)[0]
print(grads)

"""## Broadcast ##"""

a = torch.randn(4)
b = torch.randn(1)
print(a.shape, b.shape, (a*b).shape)

a = torch.randn(4, 3)
b = torch.randn(3)
print(a.shape, b.shape, (a*b).shape)

a = torch.randn(4, 1)
b = torch.randn(3,1)
print(a.shape, b.shape, (a*b).shape)

a = torch.randn(1, 4, 1)
b = torch.randn(3, 1, 1)
print(a.shape, b.shape, (a*b).shape)

"""## Permutation and Transpose ##"""

a = torch.randn(4, 3, 2)
print(a.shape)

print(a.permute(1, 2, 0).shape)

a = torch.randn(2, 3)
print(a.transpose(0,1).shape)

a = torch.randn(1, 2, 3, 4 )
print(a.transpose(1,3).shape)

"""## Special Matrix ##

identity matrix
"""

a = torch.eye(3)
print(a)

"""diagonal matrix"""

a = torch.tensor([1, 2, 3, 4])
print(torch.diag(a))

"""torch.triu() return the upper triangular matrix
torch.tril() return the lower triangular matrix
"""

a = torch.randn(4, 4)
print(a)
print(a.triu())
print(a.triu( diagonal=1))
print(a.triu( diagonal=2))
print(a.tril())